{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building a Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as sp\n",
    "import numpy as np\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required for environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/local/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/local/bin/python3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Knowledge Graph Project\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load dataset (100 documents, 20 keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords = spark.read.csv(\"./mag_cs_keywords.csv\",header=True)\n",
    "df_arxiv = spark.read.json(\"./arxiv-metadata-oai-snapshot.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = df_arxiv.select(\"id\",\"abstract\")\n",
    "keywords = df_keywords.select(\"normalizedName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\n",
    "\n",
    "# remove non ASCII characters & lowercase\n",
    "def strip_non_ascii(data_str):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c.lower() for c in data_str if 0 < ord(c) < 127)\n",
    "\n",
    "    return ''.join(stripped)\n",
    "# setup pyspark udf function\n",
    "strip_non_ascii_udf = udf(strip_non_ascii, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = abstracts.withColumn('normalized', strip_non_ascii_udf(abstracts['abstract']))\n",
    "abstracts = abstracts.select(\"id\",\"normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract keywords in document\n",
    "    - https://stackoverflow.com/questions/48869922/how-to-efficiently-check-if-a-list-of-words-is-contained-in-a-spark-dataframe\n",
    "    - https://stackoverflow.com/questions/46410887/pyspark-string-matching-to-create-new-column\n",
    "\n",
    "   \n",
    "2. Build Co-occurence matrix\n",
    "    - https://stackoverflow.com/questions/48551900/spark-generate-occurrence-matrix\n",
    "   \n",
    "   \n",
    "3. Useful tutorials\n",
    "   - https://spark.apache.org/docs/1.6.3/ml-features.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|       id|          normalized|\n",
      "+---------+--------------------+\n",
      "|0704.0001|  a fully differe...|\n",
      "|0704.0002|  we describe a n...|\n",
      "|0704.0003|  the evolution o...|\n",
      "|0704.0004|  we show that a ...|\n",
      "|0704.0005|  in this paper w...|\n",
      "|0704.0006|  we study the tw...|\n",
      "|0704.0007|  a rather non-st...|\n",
      "|0704.0008|  a general formu...|\n",
      "|0704.0009|  we discuss the ...|\n",
      "|0704.0010|  partial cubes a...|\n",
      "|0704.0011|  in this paper w...|\n",
      "|0704.0012|  recently, bruin...|\n",
      "|0704.0013|  serre obtained ...|\n",
      "|0704.0014|  in this article...|\n",
      "|0704.0015|  the pure spinor...|\n",
      "|0704.0016|  in this work, w...|\n",
      "|0704.0017|  results from sp...|\n",
      "|0704.0018|  we give a presc...|\n",
      "|0704.0019|  in this note we...|\n",
      "|0704.0020|  the shape of th...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abstracts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword List for Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list = list(keywords.select('normalizedName').toPandas()['normalizedName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords_list = keywords.select('normalizedName').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list2 = [\"fully\", \"different\", \"paper\", \"we\"] # small numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Keyword from the keyword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the function you want to return\n",
    "def extract(s):\n",
    "    all_matches = list()\n",
    "    for keyword in keywords_list2:\n",
    "        all_matches.extend(re.findall(keyword, s))\n",
    "    return ','.join(list((set(all_matches))))\n",
    "\n",
    "# Create the UDF, note that you need to declare the return schema matching the returned type\n",
    "extract_udf = udf(extract, StringType())\n",
    "\n",
    "# Apply it\n",
    "df = abstracts.withColumn('extracted', extract_udf(abstracts['normalized']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------------+\n",
      "|       id|          normalized|         extracted|\n",
      "+---------+--------------------+------------------+\n",
      "|0704.0001|  a fully differe...|we,different,fully|\n",
      "|0704.0002|  we describe a n...|                we|\n",
      "|0704.0003|  the evolution o...|                we|\n",
      "|0704.0004|  we show that a ...|                we|\n",
      "|0704.0005|  in this paper w...|          paper,we|\n",
      "|0704.0006|  we study the tw...|                we|\n",
      "|0704.0007|  a rather non-st...|paper,we,different|\n",
      "|0704.0008|  a general formu...|      we,different|\n",
      "|0704.0009|  we discuss the ...|                we|\n",
      "|0704.0010|  partial cubes a...|             paper|\n",
      "|0704.0011|  in this paper w...|          paper,we|\n",
      "|0704.0012|  recently, bruin...|          paper,we|\n",
      "|0704.0013|  serre obtained ...|          paper,we|\n",
      "|0704.0014|  in this article...|      we,different|\n",
      "|0704.0015|  the pure spinor...|   paper,different|\n",
      "|0704.0016|  in this work, w...|          we,fully|\n",
      "|0704.0017|  results from sp...|                we|\n",
      "|0704.0018|  we give a presc...|                we|\n",
      "|0704.0019|  in this note we...|                we|\n",
      "|0704.0020|  the shape of th...|                we|\n",
      "+---------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Empty Rows where no keyword exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.extracted != \"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------------+\n",
      "|       id|          normalized|         extracted|\n",
      "+---------+--------------------+------------------+\n",
      "|0704.0001|  a fully differe...|we,different,fully|\n",
      "|0704.0002|  we describe a n...|                we|\n",
      "|0704.0003|  the evolution o...|                we|\n",
      "|0704.0004|  we show that a ...|                we|\n",
      "|0704.0005|  in this paper w...|          paper,we|\n",
      "|0704.0006|  we study the tw...|                we|\n",
      "|0704.0007|  a rather non-st...|paper,we,different|\n",
      "|0704.0008|  a general formu...|      we,different|\n",
      "|0704.0009|  we discuss the ...|                we|\n",
      "|0704.0010|  partial cubes a...|             paper|\n",
      "|0704.0011|  in this paper w...|          paper,we|\n",
      "|0704.0012|  recently, bruin...|          paper,we|\n",
      "|0704.0013|  serre obtained ...|          paper,we|\n",
      "|0704.0014|  in this article...|      we,different|\n",
      "|0704.0015|  the pure spinor...|   paper,different|\n",
      "|0704.0016|  in this work, w...|          we,fully|\n",
      "|0704.0017|  results from sp...|                we|\n",
      "|0704.0018|  we give a presc...|                we|\n",
      "|0704.0019|  in this note we...|                we|\n",
      "|0704.0020|  the shape of th...|                we|\n",
      "+---------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ PMI(A,B) = \\frac{P(A,B)}{P(A)*P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(A,B) = \\frac{c((A,B), C_{\\text{pairs}})}{c(C_{\\text{pairs})}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(A) = \\frac{c(A)}{c(C)}, P(B) = \\frac{c(B)}{c(C)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{where C stands for word collection} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How to count words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'we': 82, 'different': 17, 'fully': 2, 'paper': 23})"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_temp = df.select(\"extracted\").rdd.flatMap(lambda x: (x[\"extracted\"].split(\",\")))\n",
    "corpus = corpus_temp.countByValue()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How to count cooccurence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdds = df.select(\"extracted\").rdd.map(lambda x: x['extracted'].split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('we', 'different'): 14,\n",
       "             ('we', 'fully'): 2,\n",
       "             ('different', 'fully'): 1,\n",
       "             ('paper', 'we'): 17,\n",
       "             ('paper', 'different'): 5})"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "corpus2_temp = rdds.flatMap(lambda x: combinations(x,2))\n",
    "corpus2 = corpus2_temp.countByValue()\n",
    "corpus2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How to avoid bottleneck?\n",
    "\n",
    "- Use clusters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save Result Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_temp.repartition(8).saveAsTextFile(\"./corpus.txt\")\n",
    "corpus2_temp.repartition(8).saveAsTextFile(\"./corpus2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'different': 17, 'paper': 23, 'we': 82, 'fully': 2})"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus = spark.sparkContext.textFile(\"./corpus.txt\").countByValue()\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('we', 'different'): 14,\n",
       "             ('we', 'fully'): 2,\n",
       "             ('different', 'fully'): 1,\n",
       "             ('paper', 'we'): 17,\n",
       "             ('paper', 'different'): 5})"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus2_temp = spark.sparkContext.textFile(\"./corpus2.txt\").countByValue()\n",
    "# corpus2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import math\n",
    "G.add_nodes_from(corpus.keys())\n",
    "\n",
    "for (n1, n2), cooccurence in corpus2.items():\n",
    "    pmi = math.log(cooccurence/corpus[n1]*corpus[n2])\n",
    "    G.add_edge(n1, n2, weight=pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "options = {\n",
    "    \"node_color\": \"black\",\n",
    "    \"node_size\": 50,\n",
    "    \"linewidths\": 0,\n",
    "    \"width\": 0.1,\n",
    "}\n",
    "\n",
    "PMI_Threshold = 3\n",
    "elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] >= PMI_Threshold]\n",
    "esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] < PMI_Threshold]\n",
    "\n",
    "pos = nx.spring_layout(G)  # positions for all nodes\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=50, node_color=\"black\")\n",
    "\n",
    "# edges\n",
    "nx.draw_networkx_edges(G, pos, edgelist=elarge, width=.5, linewidth=0)\n",
    "nx.draw_networkx_edges(G, pos, edgelist=esmall, width=.1, linewidth=0, alpha=0.5, style=\"dashed\")\n",
    "nx.draw_networkx_labels(G, pos, font_size=13, font_family=\"sans-serif\")\n",
    "# nx.draw_networkx(G, **options)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "# G.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"extracted\").write.mode(\"overwrite\").csv(\"./extracted.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.repartition(100000)\n",
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"extracted\").write.mode(\"overwrite\").csv(\"./extracted.csv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col, concat, lit, when\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "corpus, co_occurence, documents = Counter(), Counter(), defaultdict(set)\n",
    "df = abstracts.withColumn('extracted_words', regexp_extract('normalized', keywords_list[0], 0))\n",
    "\n",
    "keywords_list2 = [\"a\", \"describe\", \"paper\", \"algorithm\"]\n",
    "\n",
    "for i, keyword in enumerate(keywords_list[1:10],1):\n",
    "    print(\"Working on {}\".format(i))\n",
    "#     df_temp = abstracts.withColumn('extracted_temp', regexp_extract('normalized', keywords_list[1], 0))\n",
    "#     df = df_temp.withColumn('extracted_words2', concat(col('extracted_words'), lit(' '), col('extracted_temp')))\n",
    "    df_temp = df.withColumn('extracted_temp', regexp_extract('normalized', keyword, 0))\n",
    "    df = df_temp.withColumn('extracted_words', when(col('extracted_temp') != '', concat(col('extracted_words'), lit(\",\") ,col('extracted_temp'))).otherwise(col('extracted_words')))\n",
    "\n",
    "#     df_temp3 = df_temp2.withColumn('extracted_temp', regexp_extract('normalized', \"different\", 0))\n",
    "#     df_temp4 = df_temp3.withColumn('extracted_words', concat(col('extracted_words'), lit(' '), col('extracted_temp')))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #     df = df.withColumn('joined_column', sf.concat(sf.col('colname1'),sf.lit('_'), sf.col('colname2')))\n",
    "#     extracted_temp = list(df.select('extracted_words').toPandas()['extracted_words'])\n",
    "    \n",
    "    # update corpus, co_occurence, documents\n",
    "#     doc_num = 0\n",
    "#     for v in extracted_temp:\n",
    "#         if not v:\n",
    "#             continue\n",
    "#         corpus[v] += 1\n",
    "#         documents[doc_num].add(v)\n",
    "#         doc_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.write.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('extracted_words').take(20)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.withColumn('extracted_temp', regexp_extract('normalized', \"fully\", 0))\n",
    "df_temp2 = df_temp.withColumn('extracted_words', concat(col('extracted_words'), lit(' '),col('extracted_temp')))\n",
    "\n",
    "df_temp3 = df_temp2.withColumn('extracted_temp', regexp_extract('normalized', \"different\", 0))\n",
    "df_temp4 = df_temp3.withColumn('extracted_words', concat(col('extracted_words'), lit(' '), col('extracted_temp')))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = keywords.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in l[:2]:\n",
    "    print(n['normalizedName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/solving-combinatorial-problems-with-pyspark-fad433b1fca0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "\n",
    "kp = KeywordProcessor()\n",
    "keywords = ['abc']\n",
    "for keyword in keywords:\n",
    "    kp.add_keyword(keyword)\n",
    "\n",
    "def extractKeywords(menu_name, kp=kp):\n",
    "    keywords = kp.extract_keywords(menu_name)\n",
    "    return ''.join(keywords)\n",
    "    \n",
    "extractKeywords_udf = udf(extractKeywords, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn(\"extracted_keyword\", udf(abstracts[\"normalized\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "- running pyspark in jupyter\n",
    "    - https://medium.com/@Vatsal410/running-pyspark-on-an-aws-cluster-through-jupyter-notebook-fe28f6ef47a4\n",
    "- https://medium.com/@christo.lagali/run-jupyter-notebooks-with-pyspark-on-an-emr-cluster-9630ef54c4e1\n",
    "- https://towardsdatascience.com/getting-started-with-pyspark-on-amazon-emr-c85154b6b921\n",
    "- reading and writing s3\n",
    "    - https://docs.cloudera.com/runtime/7.0.2/developing-spark-applications/topics/spark-examples-of-accessing-s3-data-from-spark.html\n",
    "- textmining in apache spark\n",
    "    - https://runawayhorse001.github.io/LearningApacheSpark/textmining.html\n",
    "- https://medium.com/@serkansakinmaz/how-to-connect-amazon-s3-via-emr-based-pyspark-42707d540881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0,1,2,3,4,5,6,7,8,9]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit3209f78e23784fe7aad1ed6f75489d02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
